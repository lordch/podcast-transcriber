Now with Gemini 3.0, if we look at the benchmarks, it can do things where you ask it to code an entire
project and it'll sit there for six, seven minutes and then complete the answer for you.
Somebody's just tried a project-oiler problem and Gemini spent six minutes and solved it right
and in that time it was making multiple tool calls, getting multiple results, fixing multiple areas
right right in the code like multiple times and then it finally got to a final answer.
So we moved from a very simple like one or two turn interaction that happens relatively quickly
with some precision to now the scenario where these models are doing self-sufficient behavior
of self-complex tasks. That was Ravencuma from Google DeepMind talking about the massive
leaps in model reasoning we're seeing with Gemini 3 but the implication for product builders is
also huge because the models are fixing their own errors, builders are literally tearing out the
functionality they built just months ago ripping out the defensive coding and reshipping agent harnesses
entirely. We're seeing this with tools like Claude Code and Manus and as Raven explains
this cycle of destruction and rebuilding is happening inside Google right now.
Three days ago, Google released Gemini 3 and I was fortunate enough to be able to record an
impromptu podcast with Ravencuma from Google DeepMind. In this chat, Raven breaks down exactly why
the rapid evolution of models like Gemini 3 is changing how we build software but we go much
further than just the models. Raven and I explore the quote unquote two cultures of AI agents.
How to decide when you need strict deterministic-esque workflows versus when you should unleash high
agency autonomous systems. We also get a rare look inside the product mindset at Google. Raven shares
the backstory of building notebook LM taking us from the initial vibe coding phase all the way to
production and sharing what they learn about user control after shipping those viral audio
overviews. And for the real builders out there, we get technical. We talk about the art and science of
context engineering. Why needle in a haystack is often just a vanity metric and why building robust
eval and agent harnesses is one of the biggest competitive modes you can have right now.
This is a high signal conversation on exactly what changes when you move from simple chat bots
to complex agentic systems. I'm Hugo Mount Anderson and welcome to Vanishing Gradients.
Hello there everybody and welcome to a very special impromptu live podcast I'm doing with Raven
Kumar from DeepMind. What is up, Raven? I'm excited to be here again with you. I'm always great
being on this podcast. Yeah, so it's always so much fun to chat and it's just great to do things in
public as well. We have so many fun conversations where I learn a bunch so I love sharing it with the world
and just welcome everyone and thanks for joining. So impromptu Lee, if that's a term, this is Vanishing
Gradients, the data science machine learning and now we call it an AI podcast. The Q&A will happen
in Discord and I've put the link in the description, jump into the podcasts channel. Any questions
would be excited to answer and engage with you all. We just do that so we can follow up afterwards
as well and continue the conversation. If you like this type of stuff, please do hit like and subscribe
and share with friends. I just saw a few things about Raven. Raven is a researcher at DeepMind.
For the last decade he's focused on applied generative models, previously built bespoke,
probabilistic Bayesian models for organizations like SpaceX and more recently been working on LLM
such as Gemma and Gemini. It contributed both the products, notebook LLM project and project
Mariner. We're going to chat about all these types of things today with a focus on AI
evaluation and agent evaluation for products as well but also I woke up this morning and saw Gemma
3. So many congrats, Raven. Gemini 3. Gemini 3. I'm sorry, yes. I was too excited, yes, of course
I knew that but many congrats on Gemini 3. Thank you. So is there anything by way of introduction that
I missed or got wrong that you'd like to correct or at? No, but I'm sure most of you listen to
know you but if those you don't Hugo has been a long time calling in friend I met him on the data
science track of sci-fi and pi data where he was putting together really great tutorials and things
like that. So it's just been an interesting decade like you said working from tiny numpires now
scaling up to gazillions of parameters with the current ecosystem. It's super exciting and so that's
actually a lovely point to jump off from even just thinking about the AI ecosystem and what's
changed over the past several years. So let's rewind two and a half plus years to I think January 2023
open AI released something called function calling and everyone else soon followed but we had that
introduced the ability of elements to use tools and we got JSON mode and those things. So I'd love just
your take on the history from what's happened in the past few years with the agentic space moving
from basic function to calling to what we're seeing in the agentic landscape today. Yeah and let's
actually to frame this let's rewind a tiny bit more into the models on themselves and we'll move for it.
So I write around actually a decade ago which seems to be a good mark. Before you and I worked on LMS
LMS were really good at completing text. You would have what is it but we now call pre-trained LMS
but if you look at everything say gpt 2 blog post they talk a lot about how the LMS would take
a piece of text and it would complete it like it finished a sentence it would finish if you put it
in some Shakespeare quote it would finish it. So let's call this completion LMS. The next sort of
thing that happened was what instruction tuning is it's called a Google release a paper called
flon and then the most famous one obviously is chat tpt. These are LMS that now have the ability to
understand and follow instructions but generally the idea is they can chat like chat tpt
obviously is a famous name but we have a Gemini and many other models as well. These are probably
the models and most people use I'd say probably 99.9% of people use the models in this way. They go to
Gemini.google.com or they go to a Gemini model they say hey give me your rest of your eggs and you get
recipes back. That's a natural language instruction but something a lot of us notice at once even within
Google is you could also put LMS in this like third mode and you could say hey you have a function
calculate dates or get weather is a very canonical example and you just say in some what we
now call system instruction you you know answer to our users normal if they ask for eggs or things
like that but if they happen to ask about the weather you can respond in this very particular way.
You can create this format and emit that and that would be a great response or you'll say
if these instructions you can call this function and so now if you say hey what's the weather the
LMS can do something else instead of just hallucinating what the weather is because LMS don't have
access to the weather by themselves they could actually produce the string that the inference side or
your computer or the serving platform could detect and it could actually go then grab the weather
put it back into the context and then respond and that is what we call what is now called tool calling
or function calling so this is to go into this again there's three modalities of LMS completion
mode chat mode and now function calling slash tool calling mode and just one quick thing before I
give back to you here you go tool calling and function calling are used very interchangeably so you
might hear say tool calling or function calling for the purposes of this this podcast just treat
tool calling and function calling to be the same and will agent will mean something else in a second
another very conflated and overlapping term but we'll get the agent in a moment but here you go I'll
throw it back to you so can you see my screen I sure can that's what I see the great ducks yeah and
this is from Gemini right yes so this actually talks about how function calling works and a lot of
the time you don't need to do this yourself I think it's good to to do it some of the time particularly
when learning how these things work but one gotcha is that there are actually two LLM calls right
you pass the initial prompt and the LLM will trigger a tool call if internally it seems like
that's the correct thing to do and modern LLMs that they used to be really bad and we'll get to this
they used to be proud at figuring out when to call a tool or not now they're a lot better and we can
get to the implications of that and then if there's a tool call then you'll pass it back the result
of the tool call into the function so we see that in this schematic the other thing that I really like
is and this is an example that we use in the course and it's the weather API in fact if you say what's
the weather in Sydney what and you provide it provide the LLM with a good description for the
tool call and telling it that it actually needs latitude and longitude most LLMs will actually take
Sydney and convert it to latitude and longitude under the hood without you're asking it to because it
quote unquote knows that's what the API expects yeah to summarize that real quick there is a it is a
much more complex set of interactions between the model the user in the interaction environment then a
simple chat response like what is the capital of Australia which is a single return usually exactly
cool so let's we've got to that point in in history let's move on from and this is a tool calling
is not just weather APIs it's the ability to ping Google Drive or email or Slack or create slide decks
and all of these things coding coding is it from now now there's terminal bench we're typing and
see like him and we have Gemini CLI there's interacting with the browser if in the most extreme
scenario you have robotics so a physical robot is a tool that an LLM and Gemini robotics can use so
this space has gotten really big really fast so what happened next so I think it's basically what you
summarize and actually it's fantastic that we're talking on a Gemini three day to the folks out there
Hugo and I have pushed this session a couple times and we switched it to a podcast because every time
I talk to Hugo it's like there's so much happening so fast that if we do workshop now it's going to
be out of date and now with the Gemini Gemini three launch today you can see how that is like true I
wouldn't we wouldn't have had the time to put together workshop given it just released this morning
and that's the answer to your question here go is that Gemini 1.0 was released when I was on that
release I think it was December 2023 which was just a little bit I look actually just two years ago
honest a little less than two years ago and that had basic I don't actually know if we supported
tool calling the API like I'm starting to even forget I think we might have only released
tool calling in Gemini 1.5 in December or sorry in February 2024 and so we went from very simple
tool calls to get weather and quickly we've gone to in 17 months gone to LLM's and now are just
doing fantastic on very complex multi-step multi-hop type of agentic behavior so previously as Hugo
was saying and he showed you the graph and LLM was good at maybe like those two turns it go one side
it would get the weather and come back and get the natural language and we were amazed that it
was able to do that very well now it was Gemini 3.0 if we look at the benchmarks it can do things where
you ask it to code an entire project and it'll sit there for six seven minutes and then complete
the answer for you something that I just on how can you for instance is somebody said they just
tried a project or a problem and Gemini spent six minutes and solved it right and in that time it
was making multiple tool calls getting multiple results fixing multiple areas right right in the code
like multiple times and it finally got to a final answer so we moved from a very simple like
one or two turn interaction that happens relatively quickly with some precision to now the scenario
where these models are doing self-sufficient behavior over five minutes of self-complex tasks
it's a huge jump maybe the other way I can put this to put this at timescale when I started
learning Python it took me like a year to get the basics down and it took another four five just to
get good at coding a solid like open source project and that was me as a human learning
now Gemini went from creating a basic function 18 months ago to like probably writing entire
libraries that Hugo and I wrote overnight without our intervention like some of the simpler stuff
you and I used to write it in our open source like Beijing days absolutely and it's just moving so fast
okay yeah and I am I'll actually link to this I did a pod another podcast I do high signal with
Lance Martin last week from Langshan and he used to work he used to work on self-driving at Uber actually
and he from the work he does at Langshan he just says so much insight into how people are building today
and he mentioned two examples one is Manus where because of the just the hyper speed in model
improvement they've essentially ripped out their product five times this year and rebuilt it
particularly what we're now calling an agent harness which is essentially all the tool calls and
all the things that help something be an LMB agentic and also Claude code part of their product is
model another part is the harness around it and as the model improves they essentially rip out
the harness each time as well so it's actually changed the way we build software right yeah I think
here to that point it's even internally within Google like I've rewritten agentic harnesses like
three times in the last three years because Gemini gets so good that I don't need to check for certain
things or I can just defer into the entire of the model but on the other side we keep doing more
and more complex things so I need to build new harnesses for more more complex sets of behaviors
than ever before like it's basically here it's you're deleting the old part and you're adding the
new part and it's just the shifting window constantly yeah so we've got some great questions in the
chat from Pastor and Julian both such wonderful contributors and members of the community they're
somewhat related so I'm gonna ask them together breaking a cardinal scene of podcasting but Julian
wants to know has Gemini tree introduced any additional capabilities relative to the diagram where we
have the one we showed right and Pastor asks can we use decision nodes or something similar to change
the type of response like if it's see if it uses a tool can we try to push it to have a chat response
instead and I've done that using prompting myself for the most part but yeah I'm interested in in
those so one is Gemini if you're using basic tool calling Gemini 3.0 we'll just still do the API
that you saw is totally still so legit you can still use it with your current workflows it should just
work much better with much more complex complex tools so that's if you're using the API you're also
welcome to then build your own agentic system and we'll talk a bit about my this on top of the
right API you can you don't have to follow that diagram exactly like if you know you're some crazy
custom cool project that you want to build on the top of Gemini you can build your own flows and just
get a layer down deeper and route things your own way make multiple tool calls and incorporate images
or audio or whatever you want you have the I word this you have the standard way of just doing
functional and tool calls but if you want to build a crazy or agent that's totally bespoke you have
all the ability to do that yourself as well using the Gemini API totally and it's actually
something I have started to refer to as the two cultures of agents in AI essentially but one way
to the first way to do this is actually using workflows right and I'm going to just share my screen
talk it through for people who aren't watching but merely listening can you now see sick building
effective AI agents yeah so this is as you know the ontropic post from December last year that I
probably talk about a bit too much but it was so instructive they tell us about augmenting LLMs
to move along the agentic spectrums LLMs are stateless so introducing read and write for memory
tool calls retrieval all incredibly important things and then starting to do prompt
chaining right so we can build start building workflows together and there are all types of
different workflows we do one I really like is the evaluator optimizer one and so this is one way
to start building agents and if you do want more bespoke workflows and to dictate what your LLMs are
doing this is a great way to do it the other end of the spectrum is really you're lowering it and
it's very difficult to evaluate in a lot of ways but building large agentic systems with an end user
who maybe yourself but like a human in the loop with strong supervision so that's I've always told
people to start with these types of things but that's when you're trying to build consistent reliable
software which we're not always trying to do for example I use AI assisted coding daily and I wouldn't
call that consistent or reliable unless there's strong supervision so when you have high agency you
need strong supervision as well so I'm interested in your thoughts on kind of these two cultures
I think you're correct from this I think there's some folks these models can do so much so there's
some folks that just need just reliable I need a function I need to be called and that's all I got
then yeah using the basic API is great but now I am there's a lot of different ways we can stack these
models together and that blog post and retropic was one good way of doing it there's even more when
you consider all the ways these LLMs can be used they could be used synchronously they could use
to be used asynchronously like we could you and I could vibe code and using a use Gemini two ways we
could vibe code a podcast summarizer and we'll use it like in a dev agentic workflow and synchronously
and that's one set of product and one workflow and then that we could be coding something that takes
us podcasts and summarizes it in terms of it in a newsletter like async right and that's another
workflow with the same the same model so I think to your point there's like the dead simple case which
is great the router case but now there's many under ways to again put these models together in
different different workflows with different modalities and also to your point different risk levels
I have to say when I'm vibe coding a project at home I don't I just let the model run for five minutes
if I get an output that looks good I feel great and I'm just good to go but even in Google when I'm
coding for modeling internally like I need to really check the code unless vibe coding I'm more
precisely like instructing my model or my agents a lot of what you and I are going to come to you is
it comes down to your use case and what you need and what you can tolerate for how you decide
to structure and use these models and Evalon absolutely so we'll get to Evalon a second but I
love that you mentioned let it go for five minutes and the time is actually a really important
consideration here so I've been using Manus recently for doing research for me there's actually Manus
Mail which so let's say I'm going to do a podcast with someone I can just CC Manus Mail and it will
go off and say hey can you research all the topics we've discussed in this email thread and it will
go off and create a comprehensive summary that it would usually take me three hours to do with all
types of links and references and it returns that in about 10 minutes which is great because then I
can interact with it get it to do a bit more back and forth and so on whereas if it took several hours
that wouldn't work for me so when there's high agency having high supervision but also on a time
scale that matches the workflow what needs also. Before moving on to evaluation Pastel has another
question which is so good as models get better and better will they require fewer tools or and
orchestration to get the responses we want good responses. Oh I actually have the opposite answer so
one is maybe less orchestration actually okay so there's a couple of factors I think for if you keep
the task complexity fixed they're requiring less orchestration I actually posted about this and
linked in just like a 30 minutes ago. There's some problems with Gemini 3 that I've noticed is
take less instruction like orchestration than they did before but as they as we put them in more
and more complex scenarios sometimes you have more orchestration they match the complexity of your task
this I like food a lot so I'll give you that dumb example and I like Hugo these Australians so I'll
give you the stupid dumb example if you're going to make a tips in my mind sandwich there's really not
much destruction needed. That's worse than my Gemini my Gemini Gemini Gemafoo. My cousins are New Zealand
from the Kiwi so I was I was switched to Marmite. Yes Veggie Might sandwich it's not a complex it's
not a complex dish right so previously maybe you'd have to really precise the LLM oh go grab white bread
go grab Marmite or Veggie Might depending on which country you're in you'd have to be really
precise and you'd have to say put it together but now you can say who goes Australian make it make
him a sandwich with the correct one and it'll just know you don't have to tell it whether it's
Veggie Might or Marmite but now we're taking these models and we're having them do super complex things
so we're having it make like Michelin start and meals and so now you go back to a large prompt
because the task complexity is increased so one I'd say if you keep taxing plus to the same my experience
has been you need less destruction but I've noticed that we just push the models into more complex
scenarios so the tax plus the instruction complexity stays about the same but you're getting a more
complex task done. The other part of your question was are more or less tools needed. Actually I think
more tools are better now because now the model is able to differentiate between more tools than ever
before so previously we really only could put like one to five tools in context if you put too many
the model confused it wouldn't be able to do like a wide variety of tasks but now with especially with
the Gemini models you could put like 10 functions in you could put more than you have been able to with
the older models and it's able to differentiate and do more things and now you have a more flexible agent
that can both get the wider and make you a sandwich and it's able to do more pussy you'd have to make
a wider bot and you have to make a sandwich bot and you have to keep the context separated because the model
confused between the two modalities or the two structures that's totally agree and to just reiterate
as these models get far better at using the right tool there are less failure modes in the number
of tools we're using currently so we can increase that. I would also add a depends on your definition
of what an individual tool is because if you if you have a web search tool and a bash tool you get
really far right and you could be like oh I have a grep tool and another bash tool it's all
bash essentially right so I think it's worth noting that the other thing just slide note side note
I know we're all the spaces super obsessed with semantic search but don't forget lexical search and
hybrid search is super important and in fact you may notice that coding tools use grep for the most
part it's one of the most important tools that they have in helping you re-code. Reven I am interested
now in thinking a bit about evaluation so I think I want to get to more complex stuff but if someone's
building an agentic system for the first time what's like the minimum evaluation setup needed
before releasing it to real users. So I think I'm going to answer your question in two different ways
one is if it's the first time you're ever building a system like an agentic system I strongly
suggest you go back and just do the get weather example yourself implement it yourself with even with
an open model like a like Gemma which is my favorite obviously and just really understand what's going
on under the hood there's always this concept of leaky abstractions I have to say and LM's are a
bit of a leaky abstraction like when they work when they're in really polished interfaces like Gemma
or Marien or things like that you're getting this really clean great polished feel but you're not
seeing the complexity that's under the hood that's that got that system to that state and in particular
all the the roadblocks and the dead ends that we ran into from the modeling side and the product
building side that let us get to this really good product so if you go down and you build it yourself
you'll understand all the pieces and where they're going so my first suggestion is start with the
fundamentals go learn basic tool calling and how LM's create those generations particularly using
an open model even if you're not planning to use an open model just understand how the flow works
and I will link to a workshop that we did exactly on on this in the show notes like um
yes you want to do this because then you have a good sense of how to build a product and also your
evaluation for your more complex thing so the second part you asked is like how what should you do
before you get to your first user this is actually the same answer I'm going to have whether you're
building an agent project or non-agentic product there's always like a good tension here uh
if you want to get it good enough where your user's going to get what you're trying to do but then once
you get there you want to release it's my opinion as fast as possible because the way your users use
your model is going to be different the way you think you need to use your models because with every
product you're going to have this stage where you're building it yourself you're thinking this is
going to be really neat people are going to love it and you're coming with your own view of how
you're going to use this product but also your own biases like for instance I test a lot in
english and kodrathe because those are the two languages that I know but I don't test a lot in
like korean and japanese because I don't know those languages nearly as well so I get the basics down
with what I know and then as soon as I get to users I see all the things that I didn't think of
it's a bit scary but it helps because then you can iterate on your your product faster using a
full feedback loop from the user rather than just your own feedback loop now for a word from
our sponsor which is well me I teach a course called building AI powered software for data
scientists and software engineers with my friend and colleagues Stefan Krauchik who works on agent
force and AI agent infrastructure at sales force it's designed for people who want to go beyond
prototypes and actually ship AI powered systems the links in the show notes absolutely and I actually
what I'm trying to do is bring up just a slide you whoa I didn't mean to turn my camera off
I meant to share my screen again you came and just gave such a wonderful guest talk in our course
last week and there's one slide which is actually very relevant to this part of the conversation
so let me just bring this up can you see oh just sit cannot see anything at the moment can you see
your slide yes yeah this is the talking of last week yeah exactly so I think this speaks to
you came and talked about products that you've worked on such as notebook LM right so maybe you can
just reorient us to to this slide as well yeah so there's you've come up with a cool product idea
it's all really only in your head these days which is really great you can use you can use
Gemini or any coding tool to quickly build this prototype yourself including the visuals now and then
up and then like you really can build all aspects of a product with with AI code visuals design
website like the whole deal but this is before you have your first user now you obviously don't want
to release something that's totally broken to users so you're going to have your own e-vals sets
you're you're probably set with vibe e-vals which is you just making stuff on the spot but as he
go ahead alluded to earlier you're going to quickly also want to build an e-vall harness which is
like a structured way to run your e-vals over and over again now you'll have a structured way to run
your e-vals but you'll only have prompts for yourself like the e-vals set is really just things
that you have thought of and maybe some close friends or some close partners but it's a really small
set of what all possible if people could do but anyhow you use you're going to use this e-vals set
to help you refine the product until you get to a point where you think it's good enough
and it's reliable for your use case it could be generating podcasts like notebook LM audio
reviews which is launched last year it could be a mariner which is browsing through the internet
or on the genocide it could be answering MMLU questions whatever and maybe but my suggestion is
you're probably going to want to you want to get the users probably quicker than you sooner rather
than later the reason is your users themselves are going to teach you so many things about your product
that you never could have anticipated they're going to use it one is they could try things on the
positive side and see use case that you never thought of that you want to build it immediately so that's
one great use case for instance a notebook LM audio review something we noticed is people really
wanted more control over the podcast initial product only had one click button and you would get
you would just get what you get now people love what you got but they also wanted some fine tuning
and some customized ability and so we learned that because we launched the product on the negative
case you'll learn that the product isn't as good as you thought it was in other areas like again maybe
I keep hallucinating the difference between Vegemite and Marmite and offending your Australian and
Kiwi customers if you're in the US or Europe or Asia you probably never think about this but
you're Australian and New Zealand users are going to think about it a lot now you're going to see
these come in and you'll you can build them into your email set you can say hey make me a sandwich
and now I if I go into Hugo and I say make a Marmite sandwich I know he's going to be frustrated
as a user and it won't match what his experience is but because you've already built the harness it's
very easy for you to plug in this new data and because you have users you have a very fast feedback loop
to go from feedback evel harness sorry evel set addition to understanding your product making
the fix checking it with your harness again and then deploying it super cool so I now want to actually
want to clarify one thing I take less exception to the Marmite mistake now I realize it was Kiwi it was
for some reason when it was when I thought you thought when it was because Marmite's British as well
that was more concerning again mistaken for being a Kiwi for what that's don't get you wrong
love the British so to introduce my next question I want to actually talk we mentioned and evaluation
harness as and we've done a workshop on this actually maybe first you can just tell us what an
evaluation harness is in your words and then we can get to this and evaluation harness is just a
repeatable way to to understand your evel results so again let me sit to a food analogy because I think
it's fun and now you I know I know Hugo won't hit me for saying it think about it when you're
going to make either a veggie mine or Marmite sandwich you've never done this before you do it the
first time you take a bite and you're like oh did I put in a crisp chips in did I get the vegetable
might or buy my ratio right is vegetable might or buy my better maybe you're from another country these
are things you just buy the first time right but you'll notice that really great chefs they take notes
right they'll say that oh today I made a veggie my sandwich I put this many chips in and I tried it
and this was like it was too big or too bad that active writing it down and structuring it is a harness
it goes from you just taking a bite is vibing it to now you have some structured criteria on the
computer side and the AI side why we really care and on the agent side is we want to have a computer
run the evel for us so while we could go in and pipe in something like hey agent build me a marmite
of regiment sandwich and see whether it doesn't or not we want to do this automatically moving forward
and there's two reasons for them one is it saves a lot of time if you automate it so you're going to be
running your evel as much more often and getting many more data points vibing it takes a lot of
time because you're running one one by one and you'll spend the whole day doing it the second though
which speaks to his he goes and I statistician background is it makes it rigorous I'm with vibing it's
it's honestly really hard to get the same vibes every single time like you might have a cold one day
if you're eating the sandwich it might be you might be really full from another meal like you as a
person change your feel depending on time in the hour of the day or what you try but when you have an
evel harness everything is structured the test running is structured and the inference side is
structured the assessment side is structured and you're getting repeatable results that really are
giving you a strong signal of what's different and what's what's not great and so to drill down even
deep on this is something that we did in the workshop that I'll link to an evel harness there are
many ways to create it but one one really good way to think about it is it's a dot py script that
consists of code based checks and perhaps LLM as judge of course you don't always need judges
for a lot of things that you can check with code and a gold standard data set of what you expect users
to put into your system and what you want to get get out of the system is that how you think about it
this exactly what it um and to me now it's in since we're talking about agent evaluation it ends up
being a little bit more complex than now than a dot py file and because agents are tend to be more
complex but the basic idea is exactly the same and it's a set of like python files yeah yes exactly
and a gold data set which is yes perhaps constantly evolving depending on what users are doing so the
reason I brought up this wonderful technical report by Chroma collaborating with weights and biases
and surround generative benchmarking and what they do is essentially they give a protocol for
generating synthetically generating a gold standard data set for information retrieval and search
systems okay so I'll link to it there's Kelly has a great video in it as well 12 minute video on
YouTube and they have all the notebooks so you can do this yourself so this is for retrieval right
and search you may call it rag of course rag is such an amorphous term I think search and retrieval
is probably better here so only to that but that's how people think about it for search and they do
some wonderful work in aligning one of their judges based on trash anchors work on evalgen I think it
is so this is how like current state of the art for doing it in search I'm now wondering and you've
already let us here but how do we start thinking about how to build out evales for agents so this is
there's going to be multiple levels to it like you said one is if we go back to the chart that you
just showed earlier agentic flows are just more inherently complex than the single prompt single generation
flow so if you take a can keep using the same example now that we're stuck on it is make me what make
me tell me the recipe for a marmite sandwich or a regimen sandwich that the model just has to
respond to you and you just eval that one text generation it's actually quite straightforward but in an
agentic flow it needs to make a tool call and then it needs to read the response of that tool call
and then it needs to write an actual language response in the simplest flow so you're evelling two
things that's already twice as complex as the simple case so that's and that's just the most basic
functional in case and so you start there you're going to have to eval does it make the function call
correct and then you're going to have to eval whether the final response is also quite good and basically
that mimics what we do in search and retrieval right you don't you of course you want to end to end
evals but a lot of the time you want to look at retrieval quality and look we know how to do that
from recommended systems and from classic certs so classically recall first re-rank precision at K
something like that and then more sophisticated metrics but making sure your trebles right and then
making sure your generations right that's exactly right it's when you start building if we shift
and broaden terminology a little bit there's element systems or AI systems research a retrieval would
be another one of these things it's it's another component that's added on to an lm and of course
you want to eval both pieces independently but you want to eval them together going back to the marbite
example until I beat this with a dead horse you eval the bread separately from the regiment of marmite
you decide how to put these things together this inherently makes it more complex than just evelling one
one model by itself for a i as in particular i think another thing you should eval is is it with
different numbers of functions different complexity of functions just as a is an api use some
libraries are really easy to use i'll pick pandas pandas plotting is like a really straightforward api to
use and you can generate your plots and then a crazy hard api to use is a d3js api there's a lot going
on you have to build a lot of components yourself these are like the extreme ends of visualization if you're
trying to build a lib an agent that automatically lets a visualize spreadsheets which you totally can do
you're gonna have to eval can use the pandas api effectively can it use the can it use the d3js
AI effectively and make a decision between those two extremes it's another area of eval to think about
with with agents and then if we move on to the most complex form of agents which is agents that are not
just making function calls are making like multi-turn multi-help decisions let's say mariner it has
where it takes in multiple screenshots of the are multiple web pages and i've used the browser
and then makes actions you're then got to eval over multiple steps and over noisy images over
things that happen on the open web that don't happen in a very clean sort of lm environments you really
just have to understand your system well enough to know where the friction points or challenges could
come in and really build out evals around the areas that are most likely to cause a bad user experience
or derail your system and i know it's a very general answer but it's hard because there's so many
different it's like how do you eval at any recipe that's ever been generated we're also different
there's not just one single eval set for it every type of like dining experience even let's say and
it's the same thing now with these agentic systems there's just so many different ways you can deploy
them into so many different users the evals will be quite different absolutely and i love that you
mentioned depending on what's happening in your system because the question arises there are so many
tool calls which ones should i focus on writing evals for and all these types of things and the answer
that of course is as a lot of people are saying now look at your data and the data scientist curiosity
and mindset is essential here if you see that there's a particular tool call that doesn't work as
often as you'd like that's something to write an eval for so making sure you're aware of what happens
in your system and in fact we mentioned doing vibes first vibes can get you kind of far a link to this
but in anthropics more recent blog posts on building their multi agent research system they actually
when they looked at some initial traces they saw that the sub agents were often summarizing content
from SEO like SEO game blog posts that had crappy content and they only saw that because humans looked
at it and then recognized how to fix that so they could immediately solve that the other thing that
I found really interesting is that it looks like LLMSA agents are getting better and I didn't think we'd
get here this soon but getting better at healing their own prompts if they're given failure modes as well
so we have some form of recursion happening here yes I would say on that point Gemini 3 I think
is been really good about understanding where things are going wrong and then self-correcting
you have to remember sometimes it's not the agents it's not the LM's fault that things fail because
sometimes your environment fails the API responses and have the right token you hit a rate limit the API
fails entirely and the model needs to the model can correct for these now when previously we just
fail out to that point you should eval not only things the successful golden path but also the path
where things tend to go wrong but luckily as the earlier question alluded to now these days the
LM can handle these things on its own you don't need to retry from the programmatic side which is
something I had to do much more often just a year ago I'd have to detect it in Python whether there
was an API failure and then reprop the LM in a different way these days I'm finding myself just
throwing the response to the LM and just letting it figure it out and stripping out all of my logic for
failure detection so you also mentioned that with the complexity we have now in multi-hop agentic
systems that type of stuff that perhaps just writing a Python file isn't all you want so maybe
you can tell us a bit more about how you think about how to build these systems in infrastructure as
well so I the dumbest way I can put it is that there's so many there's so many nuances that a single
Python file won't capture it as much anymore so usually I'll have a Python file that's just test
the function calling ability then maybe I'll have another Python file that checks whether the natural
language response is good and then another one for checking failure cases in the simplest type of thing
but if you get to super complex scenarios then sometimes there's things like dynamic evals where
the eval can adapt as the LM is moving on like a system that the LM makes a call and you have another
LM potentially mock a response or things like that there's so much room now not to just I have a
static set of prompts and responses that you grade against but much more nuanced dynamical systems
especially to test ed cases and things like that that I would encourage you to try these to try these
out like really think about your evals as an edge I think hummel says a lot or a couple people say
this is quite often they're evals themselves could be a competitive advantage or something that
increases your velocity and you can come up with with deeper eval systems and single p i files these six
totally agree with that the last one real quick one on that one the last one on aging system is that
I've noticed more and more people doing this is just docker like they spit up docker containers
to just do a bunch of evals that it's more than a dot p i flower there's a whole environment for an
agent to go try bunch stuff out in the whole docker container becomes the environment they turn on
the doc container at the end after it's been a bunch of file systems changes or whatnot but that's
like a deeper eval than just a python file yeah and I do totally I think evals will be a huge part of
products modes particularly in how they allow you to do really rapid product iteration and spin
things up and spin things down as they work and don't work of course data is a huge mode as well and
will continue to be I I'm just interested as we've entered that systems like Gemini and Mariner
support just more integrated tools tool use at larger scale and I'm wondering what developers should
learn from these kinds of systems when designing their own evals one is I think this looks to what
other folks are doing is good inspiration or as a as what's possible because you can take you could
see something like Mariner where you're getting a bunch of UI understanding and multi hop and you could
think about what is it that I'm seeing with my users that could use this kind of behavior Google's
on our own way we're like proving to you that Gemini is super capable by putting it on all these
products that you're able to use for yourself but you also can take the power Gemini or Gemma and
put it into your own things that that Google doesn't know about so that's one the second is then
you can play a game where you like try and think about how would you have a eval this system and you
can use like a Mariner or anti gravity or whatever and think about just like what is it that I would have
evaled if I was to make the system myself and then build that evil for yourself as well so with Mariner
for instance since I worked with that more you can see that it takes multiple turns and it can do
multiple actions before it comes back to the user and asks potentially for for guidance or approval
or help or things like that and so you can build that into your test harness for the product you're
building is like a multi hop evil or a test to see the limits of multi hop in your your particular
system and then of course the rest of the discussion just goes to what Hugo and I have already said is
then you're going to want to that evil then becomes your like secret secret tool something you don't
necessarily publish to users but something you can use to iterate quickly on whatever it is that you're
trying to build with these L on systems so I'm wondering if you can just speak to collab agents are
really interesting now and then there's a product called stitch that I don't think a lot of people
know about so maybe you can just tell us about about what's happening in this space yeah so I'll see
these are products that I haven't worked on directly at Google so I'm going to give you my second hand
experience from my my colleagues but these are some of Google's coding coding platforms and coding tools
collab agent is integrated directly into collab you can use it you can use it now and it essentially
is like your pair programmer or software engineering data science or programming buddy that will help
create plots and pandas code or whatever you need to do in a collab so a good example now is I used
to have to write the pandas code by hand for plotting these days I just click generate and I say any
in a bar plot with these colors and I just let collab do that stitches I'm more involved at the
80 goes and codes for you I believe and these are LMS that are being used in collab is a little more
synchronous stitches more I believe async where it's going to go out and do things for you while you're
not at your your computer like it's totally different user experience and sitting down and watching
and an LLM run run on your own so something we've talked about in passing but haven't index on
explicitly more and more agents are working with images and files and speech and other multimodal
inputs so how should we think about designing agentic systems and evaluating them when we're
entering a multimodal space yeah so I suggested that earlier point it's you stick the text complexity
and you just increase it a bit more as you talk about stitches like a UI design agent for instance
for web applications and things like that and so to mix up providing code outputs and image outputs
in one sense everything stays the same you now just input instead of inputting maybe instructions and
text look at this file and edit the Python code you say look at this image and I guess with nano banana
you can actually do is you say look at this image and remove all the ducks right this is it we can build an
AI agent that removes all like the ducks from an image your inputs now would just be text it'd be the
an email set of images as well and so same as text though you would have an email set of images you
would run them through the API you would get back a set of edited images and and you would evil whether
you were happy with the quality what was output it or not now there's a couple of additional pieces
of complexity in that like working with audio and text takes more sorry audio and images takes more time
it's not as easy just to grab through it but the fundamentals stay about the same but as we talk
about before and now having an evil harness is a really good idea because you're not just dealing with
text in text out you're dealing with many more moving pieces and it becomes much more easier to handle
this when you have computer assistance through through through an evil harness super cool something we
we hinted at earlier particularly when talking about anthropics research agent what Manus does
there are a whole bunch of modern agent architectures that use orchestrators to spin up sub agents
and move context between them so I'm wondering how you think about these systems and I think
the jury's out on whether that will be the future but what challenges arise both in design and
evaluation for these types of systems yeah and so admittedly and to be a friend as well I have not
worked on one of these systems myself yet which compresses context or orchestrates with multiple sub agents
just in this way I'm going to hear I'm going to speculate though with you how some of these systems
may be designed one is when I think about e-values it's just I think at two levels is like what is
the end-to-end product experience was to look at and in this case we could surmise one big model
passes out tasks with a much smaller models and then you eventually get a thing done so on level the
product and like does it just get the thing done better than a single model does it do it and does
it do it better than an alternative architecture but on a detail level I think about all the different
pieces that need to happen for this to occur so I can think about maybe one sub agent needs to take it
maybe one of the things is it needs to compress the context does it do that that's just the one piece of
this like orchestrator system so I would break that down into and a sub e-value and so I'd have
this mix of e-values that are like end-to-end product e-values but also a lot of smaller e-values it
tests like individual parts of the the system out now the conceptually they should connect and roll
up but that's part of the e-value design mentality thinking at the different levels of which this
system works and then ensuring I have e-values that check out every particular level
makes a lot of sense and yeah I do think the context issue is so interesting now because of course
we have really large context windows but we see there's context rot as well right so if you
utilize even sub amounts of the available context we see degradation in retrieval among other things
so something that people have been doing with these systems these architectures that use orchestrators
to spin up sub agents is having an orchestrator agent essentially offload the context to a sub agent
that works with it compresses it isolates things summarizes it and passes back up the relevant stuff
there and I am interested also of course don't want to know any secret source about the things you
work on but just how you think about context engineering in your work and managing context for
agent systems so to this point you're like earlier LLM's would have these contexts well the first
of early albums would just have context limits that were my stuff going back again like the first
model I released I think I had a context window of 4k which is barely which is laughable these days
right so you'd have to do a lot of this context management manually yourself outside filtering all the
sort of stuff there are first gemma models how to context window I think of 8k yeah so again you'd
have to do all these context management things in Python but as the ecosystem move forward now we have
context windows under like 32k 128k million or more and so all this code we had that would manage
context we could just we could just start ripping out now to your point you could self context
segregation in that context itself and so I would say earlier versus Gemini weren't good as good at
keeping context over the million tokens as Gemini 3 so one way on the with going back to the original
part of this podcast is like with the with Gemini 3 coming out since we have you all harnesses
a Google for internal stuff we just run those evil harnesses again and see whether if you just
took out all this stuff for certain systems does it just work and you can with any evil harnesses
a user you could do the same thing these layers of context engineering in previous context issues just
fall away with these newer newer and better models that's the hand wave answer of as models get
better you need less and less stuff but it is very much very much true but to go back to your question
when we were launching Gemini 1.5 for instance with the million context window at the time
why would you run a lot of emails with this thing called needle and a haystack you would put it in
random fact in the middle and you just see where the context does the model remember things and where
does it seem to fail film are often and then you could use that to structure your prompts you'd maybe
put the important details at the top or the bottom depending on how this particular model model is doing
but this going back to my previous answer this was me understanding that hey we're going to do this
really long context ritual task that users want we don't want users have to worry about where to put
in the context so let me run a bunch of emails at the contact just on the context window to understand
how it works really well with this model and then I will structure the system so it puts the important
details in the right place in the context the user doesn't have to worry about it this is the end-to-end
level of thinking that helps with these really complex agentic AI systems for you to really hone in
on what is holding back your product and fix that area and then also repeatedly do that so
as new models come back you can build in that new model reduce the complexity of your product which
helps you ship faster and keep up with all the amazing advancements that are going on it's really just
like the symphony of things that are happening all at once and all together yeah and I do the points
we're taking that evaluation is just incredibly important here to guide what you do and what you're
working on I do think context rock does continue to like needle in a haystack does seem like a vanity
metric in so many ways because a lot of the time the benchmarks and I can't I haven't look at the
Gemini 1.5 paper in a while once again chrome the Chroma teams report on context rock shows that if
you just put a couple of distractors close to it like needle in haystack performance degrades
completely with higher context so this is you have to build the error and this is where I really
think folks should build around and this is another pitch to build your own own emails right like nobody
not even Google knows what you're going to do with our models and in our case for instance we could
tightly control the context and so we didn't have to worry about context aggregation but you could end
up with a different application where this is an issue like you'd have multiple users putting in
multiple things like a meeting summarizer or something and you're having all this distracting
stuff and the model performs much more poorly than let's say the notebook LM case which is you're
highly curating super high single sources that are then getting put into a Gemini's context so
I am interested in from an external take it seems as though frontier close models from the labs
just far better at tool calling and function calling and building agentic systems than open-weight
models now so I'm wondering firstly if that is a characterization you would agree with
and if so when building agents on top of local or open-weight models like Gemma what changes?
I would say it's maybe this isn't too controversial it's a wastey of a take but models that are hosted
models that they're hosted on production grade infrastructure tend to be the smoothest
easiest user experience remember when you're using models there's two things that are happening you
have a set of weights that have been trained but you also have them deployed on top of us a system
that is serving that model to you now that system could either be something like Google is providing
to you in terms of Gemini and our API or could be your own MacBook that's running it and so an
example that you you go should earlier is that with like tool calling we built a lot of things in the
API to make it a much smoother experience for you both on the model side and the API API side and
on the open-weight side you need to go do some of these things yourself so generally yes building agents
on top of these APIs will tend to give you very high performing models on very high performing
infrastructure and your life is a lot better if you're going to implement them yourself especially
open-weight side you will need to get down like a chain or an Olamma and you'll work through
the models capabilities and the inference frameworks capabilities yourself or if you're really in the
weeds you'll develop your own inference framework with its own agentic inference layer I see very
few people do this but it's like something that the most hardcore people can do you sometimes see
this with some of the coding companies I think I'm forgetting which one off the top of my head but some
of these e-editors or cursor for instance there's a really good example this cursor is building a very
deeply integrated inference stock with their with some of their VS code type of stuff yeah this the
but on the part one catch on gifty the open models as you can fine-tune them and so it's very well
understood that Gemini for instance is a very general model it does a lot of things quite well but it
is a general model even within the Gemini or the Google ecosystem there was a Gemma variant called
C2S for instance which could predict cancer and it was I hesitate to call it an agent since the word
is overloaded but it could do this thing where it understood and could understood like cancer markers
quite well and it was able now to then predict cancer markers better than Gemini could but it was
more specifically fine-tuned to do cancer markers and I wouldn't be able to do any literally anything
else you couldn't answer questions about Australia you probably can't answer questions about what to
cook for eggs or anything about your dog but it is very good at this one specialized task if you're
growing to build something that is hyper specialized fine-tuning for that case can yield you great
improvements which is really only something you can do with an open model totally and all linked to
a wonderful podcast that we did and a workshop we ran on tiny Gemma when when you released it 270
million people not 270 billion I hadn't planned we hadn't I didn't plan to asking this but could
you just give us a quick rundown I just love it so much of the Gemma the Gemma family from 270 million
and up yeah so at Google we recognize that there's like a huge need and capabilities the capabilities and
usefulness of very large models like Gemini that are just generally great and a lot of things and
really large really good at pushing the frontier of what AI can do but equally there is folks that
need models that run on device on what we say a single node like an h100 or below that's really
something you can take put it on your system fine-tuned and make it your own to serve all the other
all the other things and so in the Gemma family we have a 27b model is our largest our largest model
but it sits it fits on a single h100 or 800 but we go down to 12b 4b 1b and the smallest now is 270
m so this is something that runs on you could run on your phone quite reliably and quite easily so to
answer this like agents question again if you need your agent to work somewhere without internet or
like you're off the grid then you're gonna have to use a Gemma model like you can't use Gemini and
that's the easiest evil is it literally just will not run this isn't an agent but it's a fun example
is we have dolphin Gemma it's a fine-tuned version of Gemma that runs literally underwater with
scuba divers researchers who are like next to dolphins so you're in a notion and even if you had
started like I presume it wouldn't even go into the water so you have to use a local model to talk to
these dolphins in the ocean we have a Gemma model that specialized for that and so similarly with 270
m we made this small model that's super easily fine-tunable what you could take and train on your
own functions and make it a really great like small function calling model now admittedly if you
ran an e-mail could this thing code an entire Python code base they would miserably fail just
bring very blunt I don't even have to run the e-mail it's not going to replace your coding agent or a
Gemini CLI but if you need really great one-to-five function calls for example you can easily train a
Gemma 270 m to be really good at that one task and it'll just be great at that one task super cool
and actually one example we went through in the workshop this is a huge use case and not
insignificant part of the economy is on device gaming worlds and having NPCs NPCs able to generate
language and of course fine-tuning language models around what you want NPCs in gaming worlds to talk
like is incredibly useful I am interested in just hearing your thoughts we're going to have to wrap
up in a minute but I'm on your thoughts about what you're excited about in the agentic and
evaluation space more generally in the coming months and maybe even year so there's two two aspects
in which we talked about and I'm excited about both one is as the top my models just get better and
better Gemini 3 thankfully really today just for this podcast and I'll leave it up to you folks to
take it and see how much better it is than Gemini 2.5 I believe you will be pleasantly surprised
but this is literally the best an AI model has ever been able to do in history ever like it's really
cool to see the frontier move forward and especially so much such regular pieces so the frontier side
is very interesting to me but there's also this idea which is called like the Pareto frontier that
the small models are also getting better now they're not becoming better than the maximum limit
but for their size they're getting much much better as well and so Gemini 2.7 m and Gemini 4B are
really good examples for that are they the best models that have ever been created at this point now
Gemini 2.7 even is not the best model that's ever been created but is it the best model for its
size and its efficiency and its usability for frontearning yes I think so and it is something now that
opens up use cases so many use cases for folks that had like on device or low inference cost or
very specific needs to push the frontier in those specific areas and actually I'm very excited to
see all of that which is how people take take models of their own and push the sub front not sub
frontiers and that they're smaller but different frontiers then where the large models are going
and that is quite exciting to me so I'm also excited to see users take the Gemini models and
build their own evals see that there's areas that they could improve their performance on and then
fine tune these models to really excel for their particular use cases incredibly cool and I'm
actually excited to jump in and use Gemini 3 ASAP for many reasons but the next week of our course
is actually on function calling and building agents and I've been using Gemini 2.5 in
a lot of it so after this live stream I'm actually going to go and see how much I can change to
Gemini 3 and see what the results are so I'll report back next awesome thanks everyone for watching
and thank you Ravant for your time and wisdom and just all the great chats we have so really appreciate
it man of course all right thank you folks yeah see you all soon thanks for tuning in everybody
and thanks for sticking around until the end of the episode I would honestly love to hear from
about what resonates with you in the show what doesn't and anybody you'd like to hear me speak with
along with topics you'd like to hear more about the best way to let me know is currently on LinkedIn
and I'll put my profile in the show notes thanks once again and see you in the next episode